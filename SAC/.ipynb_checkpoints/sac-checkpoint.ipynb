{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d509ac9e-aaf9-4604-8ef8-7aad9a808af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 15:27:26.637314: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 15:27:31.405031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-19 15:27:44.386934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for g in gpus:\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186c6c5-a07d-44e9-8cca-cd1fa8d18c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3558c9-81ab-450b-ab1c-915b7ef8503b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mcpu_count()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e045426-939d-4382-9467-30263646e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "from tf_agents.environments import suite_gym, ParallelPyEnvironment, tf_py_environment\n",
    "from tf_agents.agents.sac.sac_agent import SacAgent\n",
    "from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\n",
    "from tf_agents.networks import sequential, nest_map\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.system import multiprocessing\n",
    "\n",
    "try:\n",
    "    multiprocessing.enable_interactive_mode()\n",
    "except RuntimeError as e:\n",
    "    if \"context has already been set\" not in str(e):\n",
    "        raise\n",
    "except ValueError as e:\n",
    "    if \"Multiprocessing already initialized\" not in str(e):\n",
    "        raise\n",
    "\n",
    "# Habilitar uso de GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for g in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "print(\"Usando GPU:\", tf.config.list_logical_devices('GPU'))\n",
    "\n",
    "# --- Helper para construir red critic personalizada ---\n",
    "dense = functools.partial(tf.keras.layers.Dense, activation='relu', kernel_initializer='glorot_uniform')\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "def create_sequential_critic_network(obs_units, act_units, joint_units):\n",
    "    def split(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "    obs_net   = sequential.Sequential([dense(u) for u in obs_units]) if obs_units else create_identity_layer()\n",
    "    act_net   = sequential.Sequential([dense(u) for u in act_units]) if act_units else create_identity_layer()\n",
    "    joint_net = sequential.Sequential([dense(u) for u in joint_units]) if joint_units else create_identity_layer()\n",
    "    value_layer = tf.keras.layers.Dense(1, kernel_initializer='glorot_uniform')\n",
    "    return sequential.Sequential([\n",
    "        tf.keras.layers.Lambda(split),\n",
    "        nest_map.NestMap({'observation': obs_net, 'action': act_net}),\n",
    "        nest_map.NestFlatten(),\n",
    "        tf.keras.layers.Concatenate(),\n",
    "        joint_net,\n",
    "        value_layer,\n",
    "        inner_reshape.InnerReshape(current_shape=[1], new_shape=[])\n",
    "    ], name='sequential_critic')\n",
    "\n",
    "\n",
    "def run_sac_seed(seed,\n",
    "                 #env_name = \"MountainCarContinuous-v0\",\n",
    "                 env_name=\"Pendulum-v1\",\n",
    "                 num_parallel=64*4,\n",
    "                 collect_steps=128*2,\n",
    "                 batch_size=256*4,\n",
    "                 replay_buffer_max=200_000,\n",
    "                 learning_rate=1e-4,\n",
    "                 num_iterations=1_000,#50_000,\n",
    "                 eval_interval=5_000):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)\n",
    "\n",
    "    def make_env(): return suite_gym.load(env_name)\n",
    "    py_env = ParallelPyEnvironment([make_env] * num_parallel)\n",
    "    train_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    eval_env  = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "\n",
    "    with tf.device('/GPU:0'):\n",
    "        train_step = tf.Variable(0)\n",
    "\n",
    "        actor_net  = ActorDistributionNetwork(\n",
    "            train_env.observation_spec(),\n",
    "            train_env.action_spec(),\n",
    "            fc_layer_params=(256,256)\n",
    "        )\n",
    "        critic_net1 = create_sequential_critic_network((256,256), None, (256,256))\n",
    "        critic_net2 = create_sequential_critic_network((256,256), None, (256,256))\n",
    "\n",
    "        agent = SacAgent(\n",
    "            time_step_spec=train_env.time_step_spec(),\n",
    "            action_spec=train_env.action_spec(),\n",
    "            actor_network=actor_net,\n",
    "            critic_network=critic_net1,\n",
    "            critic_network_2=critic_net2,\n",
    "            actor_optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "            critic_optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "            alpha_optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "            target_update_tau=0.005,\n",
    "            target_update_period=1,\n",
    "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "            gamma=0.99,\n",
    "            reward_scale_factor=2.0,\n",
    "            train_step_counter=train_step\n",
    "        )\n",
    "        agent.initialize()\n",
    "        agent.train = common.function(agent.train)\n",
    "\n",
    "    buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=agent.collect_data_spec,\n",
    "        batch_size=num_parallel,\n",
    "        max_length=replay_buffer_max\n",
    "    )\n",
    "    dataset = buffer.as_dataset(sample_batch_size=batch_size, num_steps=2).prefetch(tf.data.AUTOTUNE)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        train_env, agent.collect_policy, observers=[buffer.add_batch], num_steps=collect_steps\n",
    "    )\n",
    "    driver.run()  # Warm-up\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_fn():\n",
    "        experience, _ = next(iterator)\n",
    "        return agent.train(experience)\n",
    "\n",
    "    episodes = []\n",
    "    evals = []\n",
    "    ep_rewards = np.zeros(num_parallel)\n",
    "    ep_steps = np.zeros(num_parallel, dtype=int)\n",
    "    ep_count = np.zeros(num_parallel, dtype=int)\n",
    "\n",
    "    def update_episodes(time_step):\n",
    "        nonlocal ep_rewards, ep_steps, ep_count\n",
    "        rewards = time_step.reward.numpy()\n",
    "        dones = time_step.is_last().numpy()\n",
    "        ep_rewards += rewards\n",
    "        ep_steps += 1\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                ep_count[i] += 1\n",
    "                episodes.append({\n",
    "                    \"total_timesteps\": int(ep_steps[i]),\n",
    "                    \"episode_num\": int(ep_count[i]),\n",
    "                    \"episode_timesteps\": int(ep_steps[i]),\n",
    "                    \"reward\": float(ep_rewards[i])\n",
    "                })\n",
    "                ep_rewards[i] = 0.0\n",
    "                ep_steps[i] = 0\n",
    "\n",
    "    start_time = datetime.now().isoformat(timespec='seconds')\n",
    "\n",
    "    pbar = trange(num_iterations + 1, desc=f\"Seed {seed}\", dynamic_ncols=True)\n",
    "    pbar.set_postfix({\"eval_return\": \"N/A\"})\n",
    "    for step in pbar:\n",
    "        time_step, _ = driver.run()\n",
    "        update_episodes(time_step)\n",
    "        train_step_fn()\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            ts = eval_env.reset(); total = 0.0\n",
    "            while not ts.is_last():\n",
    "                action_step = agent.policy.action(ts)\n",
    "                ts = eval_env.step(action_step.action)\n",
    "                total += ts.reward.numpy().item()\n",
    "            evals.append({\n",
    "                \"at_timesteps\": int(step),\n",
    "                \"evaluation_over_1_episode\": float(total)\n",
    "            })\n",
    "            print(f\"[Step {step:>5}] Eval return = {total:.2f}\")\n",
    "            pbar.set_postfix({\"eval_return\": f\"{total:.2f}\"})\n",
    "            pbar.update(0)\n",
    "\n",
    "    data = {\n",
    "        \"experiment\": {\n",
    "            \"policy\": \"SAC\",\n",
    "            \"environment\": env_name,\n",
    "            \"seed\": seed,\n",
    "            \"start_time\": start_time\n",
    "        },\n",
    "        \"episodes\": episodes,\n",
    "        \"evaluations\": evals\n",
    "    }\n",
    "\n",
    "    folder = \"jsons\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    fname = f\"sac_{seed}_{ts}.json\"\n",
    "    path = os.path.join(folder, fname)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Saved JSON to {path}\")\n",
    "\n",
    "    return episodes, evals, agent\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    seeds = [0]\n",
    "    for s in seeds:\n",
    "        eps, evs = run_sac_seed(seed=s)\n",
    "        steps = [e['at_timesteps'] for e in evs]\n",
    "        vals = [e['evaluation_over_1_episode'] for e in evs]\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(steps, vals, marker='o')\n",
    "        plt.xlabel('Steps'); plt.ylabel('Eval Return')\n",
    "        plt.title(f'SAC Eval (seed {s})')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e9f706-ea13-4e59-90c6-af85b90bb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe19678-873f-4ba6-ae52-eacc3089139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seeds = [0]\n",
    "for s in seeds:\n",
    "    eps, evs,agent = run_sac_seed(seed=s)\n",
    "\n",
    "    # Plot evaluation returns\n",
    "    steps = [e['at_timesteps'] for e in evs]\n",
    "    vals = [e['evaluation_over_1_episode'] for e in evs]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(steps, vals, marker='o', label='Eval Return')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Eval Return')\n",
    "    plt.title(f'SAC Eval (seed {s})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    window = 100\n",
    "    rewards = [e['reward'] for e in eps]\n",
    "    avg_rewards = [np.mean(rewards[i:i+window]) for i in range(0, len(rewards) - window + 1, window)]\n",
    "    \n",
    "    plt.plot(avg_rewards)\n",
    "    plt.xlabel(\"Episode window (x100)\")\n",
    "    plt.ylabel(\"Avg reward per 100 episodes\")\n",
    "    plt.title(\"Smoothed Episode Rewards\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16666e7d-22e3-46b0-92f1-19b7026604a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.agents.sac.sac_agent.SacAgent at 0x7f34691a39d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40dcc81-bac1-4dfa-b90c-e793ca6346ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
      "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
      "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
      "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
      "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
      "WARNING:absl:Found untraced functions such as ActorDistributionNetwork_layer_call_fn, ActorDistributionNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, NormalProjectionNetwork_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n",
      "/home/pinoprie/.local/share/mamba/envs/sac-tfagents/lib/python3.8/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:497: UserWarning: Encoding a StructuredValue with type tfp.distributions.MultivariateNormalDiag_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_policypendulum1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_policypendulum1/assets\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "# after agent.initialize() and training finishes:\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "saver.save('saved_policypendulum1')   # this will create a directory 'saved_policy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f63138-5202-4bd1-b2ce-e3b532c97b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pinoprie/new_rl/RL-project/SAC/my_folderpend1.zip'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Replace 'my_folder' with your folder name\n",
    "shutil.make_archive('my_folderpend1', 'zip', 'saved_policypendulum1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713f7350-6a46-4b9c-afdd-61651626e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip -r my_folder.zip saved_policy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cfb8f2-29ce-4565-bee9-68a94b05c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7ac9b2d-63b2-4066-83c2-9ae69fd96fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.12.0\n",
      "TF-Agents: 0.17.0\n",
      "TensorFlow Probability: 0.20.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"TF-Agents:\", tf_agents.__version__)\n",
    "print(\"TensorFlow Probability:\", tfp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86169a5-de81-469b-af85-ebf5b4a62941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install \"tensorflow==2.12.0\" \"tensorflow-probability==0.20.1\" \"tf-agents==0.17.0\" \"gym==0.23.0\" \"numpy>=1.23\" \"matplotlib>=3.5\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
